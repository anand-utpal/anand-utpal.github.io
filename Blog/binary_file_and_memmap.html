<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Why should we write tokenized data to .bin (binary) (and memmap vs streaming)?</title>
  <meta name="description"
    content="Explanation of why writing tokenized data to a binary .bin file and using memmap is useful, its limitations, and modern streaming alternatives." />
  <link
    href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,300;0,400;1,300&family=JetBrains+Mono:wght@200;400&display=swap"
    rel="stylesheet">
  <style>
    :root {
      --bg: #ffffff;
      --text: #1a1a1a;
      --accent: #555555;
      --line: #eeeeee;
      --panel: #fafafa;
      --code-bg: #f4f4f4;
      --good-bg: #eaf8ef;
      --good: #257a42;
      --warn-bg: #fff1f1;
      --warn: #a22d2d;
    }

    [data-theme="dark"] {
      --bg: #121212;
      --text: #e0e0e0;
      --accent: #b8b8b8;
      --line: #2a2a2a;
      --panel: #181818;
      --code-bg: #1f1f1f;
      --good-bg: #1d2a22;
      --good: #79d79a;
      --warn-bg: #2c1d1d;
      --warn: #ff9d9d;
    }

    * {
      box-sizing: border-box;
    }

    body {
      margin: 0;
      background: var(--bg);
      color: var(--text);
      font-family: 'IBM Plex Mono', monospace;
      line-height: 1.65;
      transition: background 0.3s, color 0.3s;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 40px 20px 60px;
    }

    header {
      display: flex;
      justify-content: space-between;
      align-items: flex-start;
      gap: 16px;
      border-bottom: 1px solid var(--line);
      padding-bottom: 20px;
      margin-bottom: 28px;
    }

    .title-group h1 {
      margin: 0 0 10px;
      font-family: 'JetBrains Mono', monospace;
      font-weight: 400;
      font-size: 1.45rem;
      line-height: 1.35;
    }

    .lead {
      margin: 0;
      color: var(--accent);
      font-size: 0.95rem;
      max-width: 72ch;
    }

    .meta {
      margin-top: 10px;
      color: var(--accent);
      font-size: 0.85rem;
    }

    .header-actions {
      display: flex;
      align-items: center;
      gap: 12px;
      flex-shrink: 0;
    }

    .home-link {
      text-decoration: none;
      color: var(--accent);
      border: 1px solid var(--line);
      padding: 6px 10px;
      font-size: 0.8rem;
    }

    .switch {
      position: relative;
      display: inline-block;
      width: 50px;
      height: 24px;
    }

    .switch input {
      opacity: 0;
      width: 0;
      height: 0;
    }

    .slider {
      position: absolute;
      cursor: pointer;
      inset: 0;
      background-color: #ccc;
      transition: 0.3s;
    }

    .slider::before {
      position: absolute;
      content: "";
      height: 18px;
      width: 18px;
      left: 3px;
      bottom: 3px;
      background-color: white;
      transition: 0.3s;
    }

    input:checked+.slider {
      background-color: var(--text);
    }

    input:checked+.slider::before {
      transform: translateX(26px);
    }

    .slider.round {
      border-radius: 34px;
    }

    .slider.round::before {
      border-radius: 50%;
    }

    main {
      display: grid;
      gap: 16px;
    }

    section {
      border: 1px solid var(--line);
      background: var(--panel);
      padding: 18px;
    }

    h2 {
      font-size: 1rem;
      text-transform: uppercase;
      letter-spacing: 1.4px;
      border-left: 3px solid var(--text);
      padding-left: 12px;
      margin: 0 0 12px;
    }

    h3 {
      margin: 14px 0 8px;
      font-size: 0.95rem;
    }

    p,
    li {
      font-size: 0.92rem;
    }

    p {
      margin: 0 0 12px;
    }

    ul,
    ol {
      margin: 0 0 12px 20px;
      padding: 0;
    }

    li {
      margin-bottom: 6px;
    }

    code {
      background: var(--code-bg);
      padding: 1px 6px;
      border-radius: 5px;
      color: var(--text);
    }

    pre {
      margin: 10px 0 0;
      background: var(--code-bg);
      border: 1px solid var(--line);
      border-radius: 8px;
      padding: 12px;
      overflow-x: auto;
      font-size: 0.86rem;
      line-height: 1.5;
    }

    .badge {
      display: inline-block;
      padding: 2px 8px;
      border-radius: 12px;
      font-size: 0.75rem;
      font-weight: 600;
    }

    .good {
      background: var(--good-bg);
      color: var(--good);
    }

    .warn {
      background: var(--warn-bg);
      color: var(--warn);
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.9rem;
      margin-top: 8px;
    }

    th,
    td {
      border: 1px solid var(--line);
      padding: 10px;
      text-align: left;
      vertical-align: top;
    }

    th {
      background: var(--code-bg);
      font-weight: 400;
    }

    footer {
      text-align: center;
      margin-top: 24px;
      padding-top: 16px;
      border-top: 1px solid var(--line);
      color: var(--accent);
      font-size: 0.8rem;
    }

    @media (max-width: 768px) {
      .container {
        padding: 22px 12px 40px;
      }

      header {
        flex-direction: column;
      }

      .header-actions {
        width: 100%;
        justify-content: space-between;
      }

      .title-group h1 {
        font-size: 1.2rem;
      }

      th,
      td {
        font-size: 0.82rem;
      }
    }
  </style>
</head>

<body data-theme="dark">
  <div class="container">
    <header>
      <div class="title-group">
        <h1>Why should we write tokenized data to <code>.bin</code> (binary) (and memmap vs streaming)?</h1>
        <p class="lead">This post explains why pretokenizing data into a binary format speeds up training, where
          memmap helps, and when modern streaming pipelines are the better choice.</p>
        <div class="meta">Feb 2026 â€¢ 7 min read</div>
      </div>
      <div class="header-actions">
        <a class="home-link" href="../index.html">Back to Home</a>
        <label class="switch" aria-label="Theme toggle">
          <input id="themeToggle" type="checkbox" checked>
          <span class="slider round"></span>
        </label>
      </div>
    </header>

    <main>
      <section aria-labelledby="overview">
        <h2 id="overview">Overview: Why This Matters</h2>
        <p>Writing tokenized data to a binary <code>.bin</code> file is a critical optimization for large language model
          training. The two main wins are <strong>speed</strong> and <strong>memory efficiency</strong>.</p>
      </section>

      <section aria-labelledby="problem">
        <h2 id="problem">Problem: Raw Text in the Training Loop</h2>
        <p>If you read directly from raw <code>.txt</code> during training, each batch requires:</p>
        <ul>
          <li>Opening files repeatedly.</li>
          <li>Reading and parsing strings.</li>
          <li>Tokenizing text on the fly (CPU-bound).</li>
          <li>Padding variable-length samples each step.</li>
        </ul>
        <p>This causes GPUs to wait on CPU preprocessing, creating a serious bottleneck.</p>
      </section>

      <section aria-labelledby="binary-advantage">
        <h2 id="binary-advantage">Solution: Advantages of <code>.bin</code> + <code>memmap</code></h2>
        <ol>
          <li><strong>Pretokenize once:</strong> run tokenization one time over the full dataset.</li>
          <li><strong>Store compactly:</strong> keep token IDs in a contiguous array like <code>np.uint16</code>.</li>
        </ol>
        <h3>Why memory mapping?</h3>
        <p>Using <code>np.memmap('data/train.bin', ...)</code> allows on-disk data to be accessed like an array without
          loading all tokens into RAM.</p>
        <ul>
          <li>Trains on very large datasets with limited memory.</li>
          <li>Fast contiguous reads for batch construction.</li>
          <li>OS page cache handles efficient disk access automatically.</li>
        </ul>
        <pre><code># Example (pseudocode)
ix = torch.randint(0, len(data) - block_size, (batch_size,))
x = torch.tensor(memmap[ix : ix + block_size])
y = torch.tensor(memmap[ix + 1 : ix + 1 + block_size])
</code></pre>
      </section>

      <section aria-labelledby="limits">
        <h2 id="limits">Limitations at Scale</h2>
        <p><code>.bin</code> + <code>memmap</code> is excellent for single-node training, but scaling exposes two issues:</p>
        <ol>
          <li><strong>Weak global shuffling:</strong> random chunk sampling is not a full dataset permutation.</li>
          <li><strong>Distributed bottlenecks:</strong> one giant file is hard to serve efficiently to many machines.</li>
        </ol>
      </section>

      <section aria-labelledby="streaming">
        <h2 id="streaming">Modern Practice: Streaming Datasets</h2>
        <p>For web-scale training, teams use sharded streaming datasets in object storage (S3, GCS, etc.).</p>
        <h3>1) Shard the data</h3>
        <p>Split data into many medium-sized shard files (for example, 256 MB each) instead of one monolithic file.</p>
        <h3>2) Stream only what each worker needs</h3>
        <p>Libraries like MosaicML StreamingDataset or NVIDIA DALI fetch shards on demand, improving scalability.</p>
        <h3>3) Use efficient formats</h3>
        <p>Formats like Apache Arrow can reduce copy overhead and improve I/O performance in production settings.</p>
      </section>

      <section aria-labelledby="comparison">
        <h2 id="comparison">Quick Comparison</h2>
        <table aria-label="Data pipeline comparison">
          <thead>
            <tr>
              <th>Method</th>
              <th>Best for</th>
              <th>Key advantage</th>
              <th>Key disadvantage</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Raw <code>.txt</code></strong></td>
              <td>Prototyping only</td>
              <td>Simple setup</td>
              <td>Very slow preprocessing per step</td>
            </tr>
            <tr>
              <td><strong><code>.bin</code> + <code>memmap</code></strong></td>
              <td>Single-machine training</td>
              <td><span class="badge good">Fast I/O</span> and low RAM usage</td>
              <td><span class="badge warn">Weak shuffle</span> and limited cluster scaling</td>
            </tr>
            <tr>
              <td><strong>Streaming shards</strong></td>
              <td>Large-scale multi-node training</td>
              <td>Scales well with better shuffling</td>
              <td>More moving parts and infra complexity</td>
            </tr>
            <tr>
              <td><strong>Streaming + Arrow</strong></td>
              <td>Production-grade pipelines</td>
              <td>Low-overhead reads and transfer</td>
              <td>Higher engineering complexity</td>
            </tr>
          </tbody>
        </table>
      </section>

      <section aria-labelledby="bottom-line">
        <h2 id="bottom-line">Bottom Line</h2>
        <p><code>.bin</code> + <code>memmap</code> is ideal for single-node experiments where simplicity and throughput are
          priorities. For massive datasets and multi-node training, shift to sharded streaming pipelines for better
          scale and shuffle quality.</p>
      </section>
    </main>

    <footer>
      &copy; 2026 | Utpal Anand
    </footer>
  </div>

  <script>
    const body = document.body;
    const themeToggle = document.getElementById('themeToggle');

    function applyTheme(isDark) {
      if (isDark) {
        body.setAttribute('data-theme', 'dark');
      } else {
        body.removeAttribute('data-theme');
      }
      themeToggle.checked = isDark;
    }

    const storedTheme = localStorage.getItem('blog-theme-dark');
    if (storedTheme === null) {
      applyTheme(true);
    } else {
      applyTheme(storedTheme === 'true');
    }

    themeToggle.addEventListener('change', () => {
      const isDark = themeToggle.checked;
      applyTheme(isDark);
      localStorage.setItem('blog-theme-dark', String(isDark));
    });
  </script>
</body>

</html>
