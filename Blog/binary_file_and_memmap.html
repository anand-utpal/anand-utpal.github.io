<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Why should we write tokenized data to .bin(binary) (and memmap vs streaming)?</title>
  <meta name="description"
    content="Explanation of why writing tokenized data to a binary .bin file and using memmap is useful, its limitations, and modern streaming alternatives." />
  <style>
    :root {
      --bg: #0f1724;
      --card: #0b1220;
      --muted: #9aa4b2;
      --accent: #7c5cff;
      --glass: rgba(255, 255, 255, 0.03);
      --glass-2: rgba(255, 255, 255, 0.02);
      --text: #e6eef8;
      --success: #3ddc84;
      --danger: #ff6b6b;
      font-family: Inter, ui-sans-serif, system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial;
    }

    html,
    body {
      height: 100%
    }

    body {
      margin: 0;
      background: linear-gradient(180deg, #071021 0%, #081126 40%), var(--bg);
      color: var(--text);
      line-height: 1.55;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
      padding: 32px;
    }

    .container {
      max-width: 980px;
      margin: 0 auto;
      display: grid;
      grid-template-columns: 1fr;
      gap: 20px;
    }

    header.card {
      background: linear-gradient(180deg, rgba(255, 255, 255, 0.02), rgba(255, 255, 255, 0.01));
      border: 1px solid rgba(255, 255, 255, 0.04);
      padding: 28px;
      border-radius: 14px;
      box-shadow: 0 8px 30px rgba(2, 6, 23, 0.6);
    }

    header h1 {
      margin: 0 0 8px;
      font-size: 1.6rem;
      letter-spacing: -0.01em;
    }

    header p.lead {
      margin: 0;
      color: var(--muted);
      font-size: 0.98rem;
    }

    .grid {
      display: grid;
      grid-template-columns: 1fr;
      gap: 18px;
    }

    section.card {
      background: linear-gradient(180deg, var(--card), #071225 60%);
      padding: 22px;
      border-radius: 12px;
      border: 1px solid rgba(255, 255, 255, 0.03);
    }

    h2 {
      margin: 0 0 12px;
      font-size: 1.15rem;
      display: flex;
      align-items: center;
      gap: 10px;
    }

    .kicker {
      display: inline-block;
      background: var(--glass);
      color: var(--muted);
      padding: 6px 10px;
      border-radius: 999px;
      font-size: 0.8rem;
      margin-right: 8px;
    }

    p {
      margin: 0 0 12px
    }

    ul {
      margin: 0 0 12px 20px
    }

    li {
      margin: 6px 0
    }

    .callout {
      background: linear-gradient(90deg, rgba(124, 92, 255, 0.06), rgba(62, 128, 255, 0.02));
      border-left: 4px solid var(--accent);
      padding: 12px 16px;
      border-radius: 8px;
      color: var(--text);
    }

    pre {
      background: rgba(0, 0, 0, 0.35);
      padding: 14px;
      border-radius: 8px;
      overflow: auto;
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", "Courier New", monospace;
      font-size: 0.9rem;
      line-height: 1.45;
      border: 1px solid rgba(255, 255, 255, 0.02);
      color: #d6e3ff;
    }

    code {
      background: rgba(255, 255, 255, 0.02);
      padding: 2px 6px;
      border-radius: 6px;
      color: #cfe0ff
    }

    .two-cols {
      display: grid;
      grid-template-columns: 1fr 340px;
      gap: 18px;
    }

    @media (max-width:920px) {
      .two-cols {
        grid-template-columns: 1fr
      }
    }

    table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.95rem;
      margin-top: 8px;
      background: var(--glass-2);
      border-radius: 8px;
      overflow: hidden;
    }

    table thead th {
      text-align: left;
      padding: 12px 14px;
      background: rgba(255, 255, 255, 0.02);
      font-weight: 600;
      color: var(--muted);
    }

    table tbody td {
      padding: 12px 14px;
      border-top: 1px solid rgba(255, 255, 255, 0.02);
    }

    .badge {
      display: inline-block;
      padding: 6px 8px;
      border-radius: 8px;
      font-weight: 600;
      font-size: 0.8rem
    }

    .badge.good {
      background: rgba(61, 220, 132, 0.12);
      color: var(--success);
    }

    .badge.warn {
      background: rgba(255, 107, 107, 0.08);
      color: var(--danger);
    }

    footer {
      text-align: center;
      color: var(--muted);
      font-size: 0.9rem;
      padding: 6px 0 28px;
    }

    /* small meta row */
    .meta {
      display: flex;
      gap: 12px;
      align-items: center;
      color: var(--muted);
      font-size: 0.92rem
    }

    .meta svg {
      opacity: 0.9
    }
  </style>
</head>

<body>
  <div class="container">
    <header class="card" role="banner" aria-labelledby="page-title">
      <div class="meta" style="justify-content:space-between">
        <div style="display:flex;gap:12px;align-items:center">
          <div
            style="width:44px;height:44px;border-radius:10px;background:linear-gradient(135deg,var(--accent),#3da1ff);display:flex;align-items:center;justify-content:center;font-weight:700;color:white;">
            AI
          </div>
          <div>
            <h1 id="page-title">Why should we write tokenized data to <code>.bin</code>(binary) (and memmap vs
              streaming)</h1>
            <p class="lead">This blog explains the speed and memory-efficiency as well the limitations of memmap and
              will try to understand the modern streaming solutions for large-scale training. </p>
          </div>
        </div>
        <div style="text-align:right">
        </div>
      </div>
    </header>

    <main class="grid" role="main">
      <section class="card" aria-labelledby="intro">
        <h2 id="intro"><span class="kicker">Overview : </span>Why does this matter ?</h2>
        <p> Writing the tokenized data to a binary (<code>.bin</code>)
          file, is a critical optimization when training
          large language models. The main reasons are <strong>speed</strong> and <strong>memory efficiency</strong>.</p>


      </section>

      <section class="card" aria-labelledby="raw">
        <h2 id="raw"><span class="kicker">Problem : </span>The problem with raw text !</h2>
        <p>If you read directly from raw <code>.txt</code> during training, each batch would require:</p>
        <ul>
          <li>Opening the file.</li>
          <li>Reading text lines (strings).</li>
          <li>Running the <strong>tokenizer</strong> to convert strings into token IDs — a CPU-bound, slow process.</li>
          <li>Padding and forming variable-length batches.</li>
        </ul>
        <p>Doing that thousands of times during training results in GPUs waiting on the CPU — <strong>a massive
            bottleneck</strong>.</p>
      </section>

      <section class="card" aria-labelledby="bin">
        <h2 id="bin"><span class="kicker">Solution :</span>The advantage of <code>.bin</code> file !</h2>
        <p>Usually the text is converted into <strong>a pre-processed, ready-for-training dataset</strong>: </p>
        <ol>
          <li><strong>Converting dataset into token IDs only :</strong> run the tokenizer over the entire dataset a
            single time.</li>
          <li><strong>Store efficiently:</strong> save token IDs as a continuous array (e.g. <code>np.uint16</code>)
            which is compact — ~2 bytes per token for many vocab sizes.</li>
        </ol>

        <h3 style="margin-top:8px">Can we still load the whole dataset? No, that's why we have to go for memory mapping
          (<code>memmap</code>)</h3>
        <p>Using <code>np.memmap('data/train.bin', ...)</code> lets the OS treat the on-disk file as if it were in RAM.
          This brings several wins:</p>
        <ul>
          <li><strong>Train on huge datasets</strong> without loading everything into RAM.</li>
          <li><strong>Very fast data loading</strong> — a batch is created by reading a contiguous chunk of tokens.</li>
          <li>The OS handles (manages) file reads (operations that read data) efficiently,the OS loads the necessary
            chunks of data from disk into memory as needed.
          </li>
        </ul>

        <pre><code># Example (pseudocode)
ix = torch.randint(0, len(data) - block_size, (batch_size,))
x = torch.tensor(memmap[ix : ix + block_size])
y = torch.tensor(memmap[ix + 1 : ix + 1 + block_size])
</code></pre>
      </section>

      <div class="two-cols">
        <section class="card" aria-labelledby="limits">
          <h2 id="limits"><span class="kicker">Limitations : </span>What are the Limits of <code>.bin</code> +
            <code>memmap</code>?</h2>
          <p>While the choice of <code>.bin</code> + <code>memmap</code> is excellent for single-node training, but
            there are two key scaling issues:</p>
          <ol>
            <li><strong>Poor global shuffling</strong>: picking random chunks isn't a true dataset shuffle; true global
              shuffles of enormous files are expensive.
              <ul>
                <li>A true dataset shuffle means every example in the dataset is randomly permuted with equal
                  probability — a global reordering of all samples.</li>
                <li>For very large datasets, this requires reading, storing, and rewriting everything, which is
                  computationally expensive.</li>
                <li>By contrast, picking random chunks or shuffling within chunks only provides local randomness, not a
                  true global shuffle.</li>
              </ul>
            </li>
            <li><strong>Cluster / node scaling</strong>: when using many machines, the single large file can become an
              I/O or distribution bottleneck:
              <ul>
                <li><em>Option A</em>: copy the file to every machine — slow and wasteful.</li>
                <li><em>Option B</em>: share via NFS/object store — many clients cause heavy I/O load.
                  [Network File System (NFS) is a distributed file system protocol for shared storage.]</li>
              </ul>
            </li>
          </ol>
        </section>


        <section class="card" aria-labelledby="streaming">
          <h2 id="streaming"><span class="kicker">Modern solution </span>Streaming Datasets (Best Practice at Scale)
          </h2>
          <p>For web-scale training, the industry uses <strong>streaming</strong> and sharded datasets stored in object
            stores (S3, GCS, etc.). Key points:</p>

          <h3>1) Shards in cloud storage</h3>
          <p>Instead of one monolithic file, pre-process into thousands of shards (e.g., 256MB each), upload to S3 or
            GCS.
          </p>

          <h3>2) Smart streaming libraries</h3>
          <p>Use libraries such as MosaicML's <code>StreamingDataset</code> or NVIDIA DALI. Advantages:</p>
          <ul>
            <li>No machine downloads the entire dataset — each client streams only needed shards.</li>
            <li>Shuffling becomes straightforward: shuffle the list of shards + the contents of each shard.</li>
            <li>Scales to thousands of GPUs without copying petabytes of data.</li>
          </ul>

          <h3>3) Advanced formats: Apache Arrow</h3>
          <p>Inside shards, use a format like <strong>Apache Arrow</strong> for columnar, zero-copy reads — further
            reduces CPU/GPU transfer overhead.</p>
        </section>

        <section class="card" aria-labelledby="summary">
          <h2 id="summary"><span class="kicker">Summary :</span>A Quick Comparison !</h2>

          <table role="table" aria-label="Method comparison">
            <thead>
              <tr>
                <th>Method</th>
                <th>Best for</th>
                <th>Key advantage</th>
                <th>Key disadvantage</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Raw <code>.txt</code></strong></td>
                <td>Prototyping only</td>
                <td>Simple to create</td>
                <td>Extremely slow; CPU bottleneck</td>
              </tr>
              <tr>
                <td><strong><code>.bin</code> + <code>memmap</code></strong></td>
                <td>Single-machine training</td>
                <td><span class="badge good">Fast I/O</span> Small RAM use</td>
                <td><span class="badge warn">Weak shuffle</span>; scales poorly to clusters</td>
              </tr>
              <tr>
                <td><strong>Streaming shards</strong></td>
                <td>Large-scale training (SOTA)</td>
                <td>Scales to thousands of GPUs; good shuffling</td>
                <td>More complex setup; network dependency</td>
              </tr>
              <tr>
                <td><strong>Streaming + Arrow</strong></td>
                <td>Hyper-optimized production</td>
                <td>Zero-copy reads; minimal overhead</td>
                <td>Higher engineering complexity</td>
              </tr>
            </tbody>
          </table>
        </section>

        <section class="card" aria-labelledby="final">
          <h2 id="final"><span class="kicker">Bottom line : </span>When to use what ?</h2>
          <p>The<code>.bin</code> + <code>memmap</code> approach is <strong>excellent</strong> for single-node
            experiments and for keeping your GPUs busy without heavy engineering. For truly massive datasets and cluster
            training, evolve to sharded streaming datasets (optionally with Arrow) for scalability and better shuffle.
          </p>


        </section>

    </main>


  </div>
</body>

</html>
